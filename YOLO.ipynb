{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "YOLO.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/murthy95/colab/blob/master/YOLO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpQ3phdC92ua",
        "colab_type": "text"
      },
      "source": [
        "##You Only Look Once: Unified, Real-Time Object Detection\n",
        "\n",
        "[here](https://arxiv.org/pdf/1506.02640.pdf)\n",
        "\n",
        "###Summary\n",
        "\n",
        "\n",
        "*   The network outputs a tensor of shape (7, 7, 10+n_classes). At each box in the 7x7 grid the network predicts two bounding boxes each box predicts noramlized offset of cooridnates of the bounding box from the grid, normalized width and height of detection box and objectivity. The class logits, vector of size n_classes corresponds to remaining channels\n",
        "*   Loss function has three terms which includes classfiication loss, localization loss, object loss and no object loss \n",
        "*   All the boxes with class objectivity score  < 0.25 are ignored \n",
        "*   The right box corresponding to the box in the label is identified using non maximal supression where the predictions of same class with iou greater than threshold are suppressed and only the box with maximum objectivity is tacken as prediction and identified as match to evalaute loss\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ze1OznBjoCiA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#dataset loading\n",
        "n_classes = 20\n",
        "n_boxes = 2\n",
        "\n",
        "#transform bounding boxes when resizing images\n",
        "\n",
        "class rescale_bbox(object):\n",
        "  '''reshapes the boudning box coordinates in coordination with the input image\n",
        "  Args:\n",
        "      image_size(int, tuple) accepts int or tuple of ints: image shape and width\n",
        "   \n",
        "  '''\n",
        "  def __init__(self, image_shape, transform):\n",
        "    self.resize = image_shape\n",
        "    self.factor = (1, 1)\n",
        "    self.transform = transform\n",
        "    self.class_dict = {'aeroplane' : 0,\n",
        "                    'bicycle' : 1,\n",
        "                    'bird' : 2,\n",
        "                    'boat' : 3,\n",
        "                    'bottle' : 4,\n",
        "                    'bus' : 5,\n",
        "                    'car' : 6,\n",
        "                    'cat' : 7,\n",
        "                    'chair' : 8,\n",
        "                    'cow' : 9,\n",
        "                    'diningtable' : 10,\n",
        "                    'dog' : 11,\n",
        "                    'horse' : 12,\n",
        "                    'motorbike' : 13,\n",
        "                    'person' : 14,\n",
        "                    'pottedplant' : 15,\n",
        "                    'sheep' : 16,\n",
        "                    'sofa' : 17,\n",
        "                    'train' : 18,\n",
        "                    'tvmonitor' : 19}\n",
        "    \n",
        "  def __call__(self, img, target):\n",
        "    \n",
        "    label = target\n",
        "    shape_x = int(label['annotation']['size']['width'])\n",
        "    shape_y = int(label['annotation']['size']['height'])\n",
        "    self.factor = (self.resize/shape_x, self.resize/shape_y)\n",
        "    objects = label['annotation']['object']\n",
        "    transformed_boxes = []\n",
        "    try:\n",
        "      for obj in objects:\n",
        "        transformed_boxes.append(self.return_transformed_box(obj))\n",
        "    except:\n",
        "      transformed_boxes.append(self.return_transformed_box(objects))\n",
        "    return self.transform(img), transformed_boxes\n",
        "    \n",
        "  def return_transformed_box(self, obj):\n",
        "    class_id = self.class_dict[obj['name']]\n",
        "    bndbox = [int(obj['bndbox']['xmin']),\n",
        "             int(obj['bndbox']['xmax']),\n",
        "             int(obj['bndbox']['ymin']),\n",
        "             int(obj['bndbox']['ymax']),]\n",
        "    \n",
        "    return ((bndbox[0] + bndbox[1])/2 * self.factor[0],\\\n",
        "            (bndbox[2] + bndbox[3])/2 * self.factor[1],\\\n",
        "            (bndbox[1] - bndbox[0]) * self.factor[0],\\\n",
        "            (bndbox[3] - bndbox[2]) * self.factor[1], class_id)\n",
        "       \n",
        "    \n",
        "#loading data\n",
        "import torchvision\n",
        "import torch\n",
        "transform = torchvision.transforms.Compose(\n",
        "    [torchvision.transforms.Resize(448),\n",
        "     torchvision.transforms.ToTensor(),\n",
        "     torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = torchvision.datasets.VOCDetection(root='./data', year='2007', \n",
        "                                             image_set='train',\n",
        "                                        download=True, transforms=rescale_bbox(448, transform))\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.VOCDetection(root='./data', year='2007', \n",
        "                                             image_set='val',\n",
        "                                       download=True, transforms=rescale_bbox(448, transform))\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
        "                                         shuffle=False, num_workers=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SRQI312CaS3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#network definition \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class conv_pool_block(nn.Module):\n",
        "  def __init__(self, kernel_sizes, in_maps, out_maps, n_repeat, pooling=False):\n",
        "    assert len(kernel_sizes) == len(out_maps), 'Inconsistent kernel and filter lengths'\n",
        "     \n",
        "    super(conv_pool_block, self).__init__()\n",
        "    self.layers = []\n",
        "    for _ in range(n_repeat):\n",
        "      self.layers.append(nn.Conv2d(in_maps, out_maps[0], \n",
        "                                    kernel_sizes[0], stride=1, \n",
        "                                    padding = kernel_sizes[0]//2))\n",
        "      for i in range(1, len(out_maps)):\n",
        "        self.layers.append(nn.Conv2d(out_maps[i-1], out_maps[i], \n",
        "                                    kernel_sizes[i], stride=1, \n",
        "                                    padding = kernel_sizes[i]//2))\n",
        "    if pooling:\n",
        "      self.layers.append(nn.MaxPool2d(2,2))\n",
        "      \n",
        "    self.model = nn.Sequential(*self.layers)\n",
        "      \n",
        "  def forward(self, x):\n",
        "    return self.model(x)\n",
        "    \n",
        "\n",
        "class yolonet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(yolonet, self).__init__()\n",
        "    self.model = nn.Sequential(nn.Sequential(nn.Conv2d(3, 64, 7,stride=2, padding=3),\n",
        "                              nn.MaxPool2d(2, stride=2)), \n",
        "                              conv_pool_block([3], 64, [192], 1, pooling=True), \n",
        "                              conv_pool_block([1, 3, 1, 3], 192, \n",
        "                                  [128, 256, 256, 512],\n",
        "                                  1, pooling=True),\n",
        "                              conv_pool_block([1, 3], 512, \n",
        "                                  [256, 512], 4), \n",
        "                              conv_pool_block([1, 3], 512, \n",
        "                                  [512, 1024],\n",
        "                                  1, pooling=True), \n",
        "                              conv_pool_block([1, 3], 1024, \n",
        "                                  [512, 1024], 2),\n",
        "                              conv_pool_block([3], 1024, \n",
        "                                  [1024], 1), \n",
        "                              nn.Conv2d(1024, 1024, 3, stride=2, padding=1),\n",
        "                              conv_pool_block([3, 3], 1024, \n",
        "                                  [1024, 1024], 1))\n",
        "    self.Linear1 = nn.Linear(49*(n_boxes*5 + n_classes), 4096)\n",
        "    self.Linear2 = nn.Linear(4096, 1470)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    x = self.model(x)\n",
        "    x = x.view(x.size(0), -1)\n",
        "    x = self.Linear1(x)\n",
        "    x = self.Linear2(x)\n",
        "    return x.view((x.size(0), 30, 7, 7))\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fw1Yp1X2HoGI",
        "colab_type": "code",
        "outputId": "ca9e9653-1cc3-4912-e17f-5d153d5d871e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        }
      },
      "source": [
        "#creating a network instance and moving to gpu\n",
        "from torchsummary import summary\n",
        "net = yolonet()\n",
        "\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "net = net.to(device)\n",
        "summary(net,(3, 448,448))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 224, 224]           9,472\n",
            "         MaxPool2d-2         [-1, 64, 112, 112]               0\n",
            "            Conv2d-3        [-1, 192, 112, 112]         110,784\n",
            "         MaxPool2d-4          [-1, 192, 56, 56]               0\n",
            "   conv_pool_block-5          [-1, 192, 56, 56]               0\n",
            "            Conv2d-6          [-1, 128, 56, 56]          24,704\n",
            "            Conv2d-7          [-1, 256, 56, 56]         295,168\n",
            "            Conv2d-8          [-1, 256, 56, 56]          65,792\n",
            "            Conv2d-9          [-1, 512, 56, 56]       1,180,160\n",
            "        MaxPool2d-10          [-1, 512, 28, 28]               0\n",
            "  conv_pool_block-11          [-1, 512, 28, 28]               0\n",
            "           Conv2d-12          [-1, 256, 28, 28]         131,328\n",
            "           Conv2d-13          [-1, 512, 28, 28]       1,180,160\n",
            "           Conv2d-14          [-1, 256, 28, 28]         131,328\n",
            "           Conv2d-15          [-1, 512, 28, 28]       1,180,160\n",
            "           Conv2d-16          [-1, 256, 28, 28]         131,328\n",
            "           Conv2d-17          [-1, 512, 28, 28]       1,180,160\n",
            "           Conv2d-18          [-1, 256, 28, 28]         131,328\n",
            "           Conv2d-19          [-1, 512, 28, 28]       1,180,160\n",
            "  conv_pool_block-20          [-1, 512, 28, 28]               0\n",
            "           Conv2d-21          [-1, 512, 28, 28]         262,656\n",
            "           Conv2d-22         [-1, 1024, 28, 28]       4,719,616\n",
            "        MaxPool2d-23         [-1, 1024, 14, 14]               0\n",
            "  conv_pool_block-24         [-1, 1024, 14, 14]               0\n",
            "           Conv2d-25          [-1, 512, 14, 14]         524,800\n",
            "           Conv2d-26         [-1, 1024, 14, 14]       4,719,616\n",
            "           Conv2d-27          [-1, 512, 14, 14]         524,800\n",
            "           Conv2d-28         [-1, 1024, 14, 14]       4,719,616\n",
            "  conv_pool_block-29         [-1, 1024, 14, 14]               0\n",
            "           Conv2d-30         [-1, 1024, 14, 14]       9,438,208\n",
            "  conv_pool_block-31         [-1, 1024, 14, 14]               0\n",
            "           Conv2d-32           [-1, 1024, 7, 7]       9,438,208\n",
            "           Conv2d-33           [-1, 1024, 7, 7]       9,438,208\n",
            "           Conv2d-34           [-1, 1024, 7, 7]       9,438,208\n",
            "  conv_pool_block-35           [-1, 1024, 7, 7]               0\n",
            "           Linear-36                 [-1, 4096]     205,524,992\n",
            "           Linear-37                 [-1, 1470]       6,022,590\n",
            "================================================================\n",
            "Total params: 271,703,550\n",
            "Trainable params: 271,703,550\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 2.30\n",
            "Forward/backward pass size (MB): 136.32\n",
            "Params size (MB): 1036.47\n",
            "Estimated Total Size (MB): 1175.09\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnh8RSgWVCTM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#defining loss fuction\n",
        "lambda_coord = 0.1\n",
        "lambda_noobj = 0.5\n",
        "'''\n",
        "classification loss\n",
        "assuming we get labels and predicitons after performing \n",
        "non maximal supression using iou evaluation\n",
        "preds : torch tensor consisting of center x, center y w, h normalized , \n",
        "         objectivity, 20 class probabilities \n",
        "'''\n",
        "center_x_preds, center_y_preds, w_preds,\\\n",
        "h_preds, objectivity, class_logits = torch.split(prediction, [1,1,1,1,1,20], dim=1)\n",
        "classification_criterion = \n",
        "localization_criterion = \n",
        "\n",
        "def yolo_loss(nn.Module):\n",
        "  lambda_coord = 0.5\n",
        "  lambda_noobj = 0.5\n",
        "  def __init__(self):\n",
        "    super(yolo_loss, self).__init__()\n",
        "    self.classification_loss = nn.CrossEntropyLoss()\n",
        "    self.bbloss_x = nn.MSELoss()\n",
        "    self.bbloss_y = nn.MSELoss()\n",
        "    self.bbloss_w = nn.MSELoss()\n",
        "    self.bbloss_h = nn.MSELoss()\n",
        "    self.noobjloss = nn.MSELoss()\n",
        "    self.objloss = nn.MSELoss()\n",
        "    \n",
        "  def forward(self, x, target):\n",
        "    bbx, bby, bbw, bbh, logits, obj = torch.split(x, [1,1,1,1,1,20], dim=1)\n",
        "    _bbx, _bby, _bbw, _bbh, label = torch.split(target, 1, dim=1)\n",
        "    return self.classification_loss(logits, label) + \\\n",
        "              self.lambda_coord * self.bbloss_x(bby, _bby) + \\\n",
        "              self.lambda_coord * self.bbloss_w(F.sqrt(bbw), F.sqrt(_bbw)) + \\\n",
        "              self.lambda_coord * self.bbloss_h(F.sqrt(bbw), F.sqrt(_bbw)) + \\\n",
        "              self.objloss(obj, torch.ones(obj.size(0)))\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slFzSysKVDGG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#preparing the labels using non maximum supression \n",
        "#input assumed to be pytorch tensor\n",
        "image_width = 448\n",
        "image_height = 448\n",
        "def IoU(bb1, bb2):\n",
        "  '''\n",
        "  bb1 : input bounding box 1 the coordinates corresponds to \n",
        "        center_x center_y width height\n",
        "  bb2 : bounding box 2 coordinates in the same order as bb1\n",
        "  '''\n",
        "  minx = min(bb1[0] - bb1[2] // 2, bb2[0] - bb2[2] // 2)\n",
        "  maxx = max(bb1[0] + bb1[2] // 2, bb2[0] + bb2[2] // 2)\n",
        "  miny = min(bb1[1] - bb1[3] // 2, bb2[1] - bb2[3] // 2)\n",
        "  maxy = max(bb1[1] + bb1[3] // 2, bb2[1] + bb2[3] // 2)\n",
        "  \n",
        "  diffx = (maxx - minx) - (bb1[2] + bb2[2]) \n",
        "  diffy = (maxy - miny) - (bb1[3] + bb2[3])\n",
        "  if diffx < 0  and diffy < 0 :\n",
        "      intersection = diffx * diffy\n",
        "      return intersection / (bb1[2] * bb1[3] + bb2[2] * bb2[3] - intersection) \n",
        "  return 0\n",
        "  \n",
        "def get_labels_with_predicitons(ground_truth, pred_tensor):\n",
        "  '''\n",
        "  ground truth : a tensor with shape length of boxes and columns \n",
        "                equal to 5 corresponding to bounding box params and class label\n",
        "  pred_tensor  : a tensor with shape 30, 7, 7\n",
        "  '''\n",
        "  #need to make a grid of values corresponding to coordinates\n",
        "  x = torch.tensor(torch.arange(7))\n",
        "  y = torch.tensor(torch.arange(7))\n",
        "  grid_x, grid_y = torch.meshgrid(x, y)\n",
        "  pred_tensor = pred_tensor.view((-1, 30))\n",
        "  preds1, preds2, _ = torch.split(pred_tensor, [5, 5, 20], dim=1)\n",
        "  \n",
        "  x_min_hat = pred_tensor[:, 0]"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}